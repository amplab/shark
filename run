#!/bin/bash

# This file is used to launch Shark on the master.
export SCALA_VERSION=2.9.2

# Figure out where the framework is installed
FWDIR="$(cd `dirname $0`; pwd)"

export SHARK_HOME="$FWDIR"

# Load environment variables from conf/shark-env.sh, if it exists
if [ -e $SHARK_HOME/conf/shark-env.sh ] ; then
  . $SHARK_HOME/conf/shark-env.sh
fi

if [[ -n "${SCALA_HOME:-}" && ! -f "${SCALA_HOME:-}/lib/scala-library.jar" ]] ; then
  echo "Cannot find $SCALA_HOME/lib/scala-library.jar."
  echo "Are you sure your SCALA_HOME is set correctly?"
  echo "SCALA_HOME = $SCALA_HOME"
  exit 1
fi

# Hive related section.
if [ "x$HIVE_HOME" == "x" ] ; then
  echo "No HIVE_HOME specified. Please set HIVE_HOME."
  exit 1
fi

if [ -z "${HIVE_VERSION:-}" ]; then
  HIVE_VERSION="0.9.0"
fi

if [ ! -f "$HIVE_HOME/lib/hive-exec-$HIVE_VERSION.jar" ] ; then
  echo "Cannot find $HIVE_HOME/lib/hive-exec-$HIVE_VERSION.jar."
  echo "Are you sure your HIVE_HOME is set correctly?"
  echo "HIVE_HOME = $HIVE_HOME"
  exit 1
fi


if [ -n "$MASTER" ] ; then
  if [ -z $SPARK_HOME ] ; then
    echo "No SPARK_HOME specified. Please set SPARK_HOME for cluster mode."
    exit 1
  fi
fi

# Check for optionally specified configuration file path
if [ "x$HIVE_CONF_DIR" == "x" ] ; then
    HIVE_CONF_DIR="$HIVE_HOME/conf"
fi

if [ -f "${HIVE_CONF_DIR}/hive-env.sh" ]; then
  . "${HIVE_CONF_DIR}/hive-env.sh"
fi

# Add Shark jars.
for jar in `find $SHARK_HOME/lib -name '*jar'`; do
  SPARK_CLASSPATH+=:$jar
done

# sbt-specific jar directories.
if [ -d "$SHARK_HOME/lib_managed/jars" ]; then
  for jar in `find $SHARK_HOME/lib_managed/jars -name '*jar'`; do
    SPARK_CLASSPATH+=:$jar
  done
fi

if [ -d "$SHARK_HOME/lib_managed/bundles" ]; then
  for jar in `find $SHARK_HOME/lib_managed/bundles -name '*jar'`; do
    SPARK_CLASSPATH+=:$jar
  done
fi

# Add Hive jars.
for jar in `find $HIVE_HOME/lib -name '*jar'`; do
  # Ignore the logging library since it has already been included with the Spark jar.
  if [[ "$jar" != *slf4j* ]]; then
    SPARK_CLASSPATH+=:$jar
  fi
done

SPARK_CLASSPATH+=:$HIVE_CONF_DIR

if [ -d "$SHARK_HOME/target" ]; then
  # Build up classpath in development mode
  if [ -f "$SHARK_HOME/target/scala-$SCALA_VERSION/shark_$SCALA_VERSION-0.2.jar" ] ; then
    SPARK_CLASSPATH+=":$SHARK_HOME/target/scala-$SCALA_VERSION/shark_$SCALA_VERSION-0.2.jar"
  else
    SPARK_CLASSPATH+=":$SHARK_HOME/target/scala-$SCALA_VERSION/classes"
  fi

  SPARK_CLASSPATH+=":$SHARK_HOME/target/scala-$SCALA_VERSION/test-classes"
fi

# Use Hadoop configuration from $HADOOP_CONF_DIR or if it is not set from $HADOOP_HOME/conf.
if [ -z "${HADOOP_CONF_DIR:-}" ]; then
  if [ -z "${HADOOP_HOME:-}" ] ; then
    echo "No HADOOP_HOME specified. Shark will run in local-mode"
  else
    HADOOP_CONF_DIR="$HADOOP_HOME/conf"
  fi
fi

if [ -n "${HADOOP_CONF_DIR:-}" ]; then
  SPARK_CLASSPATH+=:$HADOOP_CONF_DIR
fi

# TODO(rxin): Check aux classpath and aux java opts.
#CLASSPATH=${CLASSPATH}:${AUX_CLASSPATH}

export SPARK_CLASSPATH
export CLASSPATH+=$SPARK_CLASSPATH # Needed for spark-shell

export SPARK_JAVA_OPTS+=" $TEST_JAVA_OPTS"


if [ "x$SHARK_MASTER_MEM" == "x" ] ; then
  SHARK_MASTER_MEM="512m"
fi

# Set JAVA_OPTS to be able to load native libraries and to set heap size
JAVA_OPTS+="$SPARK_JAVA_OPTS"
JAVA_OPTS+=" -Djava.library.path=$SPARK_LIBRARY_PATH"
JAVA_OPTS+=" -Xms$SHARK_MASTER_MEM -Xmx$SHARK_MASTER_MEM"
export JAVA_OPTS

# In case we are running Ant
export ANT_OPTS=$JAVA_OPTS

if [ "x$RUNNER" == "x" ] ; then
  if [ "$SHARK_LAUNCH_WITH_JAVA" == "1" ]; then
    if [ -n "$SCALA_HOME" ]; then
      CLASSPATH+=":$SCALA_HOME/lib/scala-library.jar"
      CLASSPATH+=":$SCALA_HOME/lib/scala-compiler.jar"
      CLASSPATH+=":$SCALA_HOME/lib/jline.jar"
    fi
    if [ -n "$JAVA_HOME" ]; then
      RUNNER="${JAVA_HOME}/bin/java"
    else
      RUNNER=java
    fi
    # The JVM doesn't read JAVA_OPTS by default so we need to pass it in
    EXTRA_ARGS="$JAVA_OPTS"
  else
    if [ -z "$SCALA_HOME" ]; then
      echo "Neither SHARK_LAUNCH_WITH_JAVA nor SCALA_HOME are set" >&2
      exit 1
    fi
    SCALA=${SCALA_HOME}/bin/scala
    RUNNER="$SCALA -cp \"$CLASSPATH\""
    EXTRA_ARGS=""
  fi
fi

exec $RUNNER $EXTRA_ARGS "$@"
